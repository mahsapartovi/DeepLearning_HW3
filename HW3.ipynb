{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bec9f64b-12a5-45ef-99e5-babe86f520ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in ./.local/lib/python3.11/site-packages (4.46.2)\n",
      "Requirement already satisfied: filelock in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in ./.local/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.local/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in ./.local/lib/python3.11/site-packages (from transformers) (0.20.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.11/site-packages (from transformers) (4.67.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.local/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /software/slurm/spackages/linux-rocky8-x86_64/gcc-12.2.0/anaconda3-2023.09-0-3mhml42fa64byxqyd5fig5tbih625dp2/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "import requests\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import transformers\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from transformers import BertModel, BertTokenizerFast, AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "\n",
    "\n",
    "devices = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a9da226-0fba-4f90-ab47-de4958edc8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loading_data(path): \n",
    "    with open(path, 'rb') as f:\n",
    "        raw_data = json.load(f)\n",
    "    number_q = 0\n",
    "    number_pos = 0\n",
    "    number_imp = 0\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    \n",
    "    for group in raw_data['data']:\n",
    "        for paragraph in group['paragraphs']:\n",
    "            context = paragraph['context']\n",
    "            for qa in paragraph['qas']:\n",
    "                question = qa['question']\n",
    "                number_q  = number_q  +1\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context.lower())\n",
    "                    questions.append(question.lower())\n",
    "                    answers.append(answer)\n",
    "    return number_imp, number_q, number_pos, contexts, questions, answers\n",
    "\n",
    "\n",
    "number_imp, number_q, number_pos,training_contexts, train_questions, training_answers = loading_data('spoken_train-v1.1.json')\n",
    "number_questions  = number_q\n",
    "number_imposible  = number_imp\n",
    "number_posible = number_pos\n",
    "\n",
    "\n",
    "number_imp, number_q, number_pos, valid_contexts, valid_questions, valid_answers = loading_data('spoken_test-v1.1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c6fbfc3-71d3-49bb-81d2-a9f1d59ead27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_answer_end(answers, contexts):\n",
    "    for answer, context in zip(answers, contexts):\n",
    "        answer['text'] = answer['text'].lower()\n",
    "        answer['answer_end'] = answer['answer_start'] + len(answer['text'])\n",
    "add_answer_end(valid_answers, valid_contexts)\n",
    "add_answer_end(training_answers, training_contexts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "10f1c36d-27ce-4115-9f15-6581e72c551e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_PATH = \"bert-base-uncased\"\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "doc_stride = 128\n",
    "training_contexts_truncated=[]\n",
    "tokenizerFast = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
    "pad_on_right = tokenizerFast.padding_side == \"right\"\n",
    "\n",
    "for i in range(len(training_contexts)):\n",
    "    if(len(training_contexts[i])>512):\n",
    "        answer_start=training_answers[i]['answer_start']\n",
    "        answer_end=training_answers[i]['answer_start']+len(training_answers[i]['text'])\n",
    "        midpoint=(answer_start+answer_end)//2\n",
    "        para_start=max(0,min(midpoint - MAX_LENGTH//2,len(training_contexts[i])-MAX_LENGTH))\n",
    "        para_end = para_start + MAX_LENGTH \n",
    "        training_contexts_truncated.append(training_contexts[i][para_start:para_end])\n",
    "        training_answers[i]['answer_start']=((512/2)-len(training_answers[i])//2)\n",
    "    else:\n",
    "        training_contexts_truncated.append(training_contexts[i])\n",
    "\n",
    "training_encodings_fast = tokenizerFast(train_questions, training_contexts_truncated,  max_length = MAX_LENGTH,truncation=True,\n",
    "        stride=doc_stride,\n",
    "        padding=True)\n",
    "valid_encodings_fast = tokenizerFast(valid_questions,valid_contexts,  max_length = MAX_LENGTH, truncation=True,stride=doc_stride,\n",
    "        padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f3907815-f1f9-4118-893b-ac602db63c90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def answer_start_and_end_train(idx):\n",
    "    start = 0\n",
    "    end = 0\n",
    "    answer_encoding_fast = tokenizerFast(training_answers[idx]['text'],  max_length = MAX_LENGTH, truncation=True, padding=True)\n",
    "    for a in range( len(training_encodings_fast['input_ids'][idx]) -  len(answer_encoding_fast['input_ids']) ):\n",
    "        match = True\n",
    "        for i in range(1,len(answer_encoding_fast['input_ids']) - 1):\n",
    "            if (answer_encoding_fast['input_ids'][i] != training_encodings_fast['input_ids'][idx][a + i]):\n",
    "                match = False\n",
    "                break\n",
    "            if match:\n",
    "                start = a+1\n",
    "                end = a+i+1\n",
    "                break\n",
    "    return(start, end)\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "ctr = 0\n",
    "for h in range(len(training_encodings_fast['input_ids'])):\n",
    "    s, e = answer_start_and_end_train(h)\n",
    "    start_positions.append(s)\n",
    "    end_positions.append(e)\n",
    "    if s==0:\n",
    "        ctr = ctr + 1\n",
    "\n",
    "training_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "valid_encodings_fast.update({'start_positions': start_positions, 'end_positions': end_positions})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "efad9db7-6ee7-4c0e-bacc-36252c2d1722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InputDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            'input_ids': torch.tensor(self.encodings['input_ids'][i]),\n",
    "            'token_type_ids': torch.tensor(self.encodings['token_type_ids'][i]),\n",
    "            'attention_mask': torch.tensor(self.encodings['attention_mask'][i]),\n",
    "            'start_positions': torch.tensor(self.encodings['start_positions'][i]),\n",
    "            'end_positions': torch.tensor(self.encodings['end_positions'][i])\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "    \n",
    "training_dataset = InputDataset(training_encodings_fast)\n",
    "valid_dataset = InputDataset(valid_encodings_fast)\n",
    "\n",
    "training_data_loader = DataLoader(training_dataset, batch_size=16, shuffle=True)\n",
    "valid_data_loader = DataLoader(valid_dataset, batch_size=1)\n",
    "\n",
    "bert_model = BertModel.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "760e1f69-8377-4320-9467-9f25b4396fae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QAModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QAModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.drop_out = nn.Dropout(0.1)\n",
    "        self.l1 = nn.Linear(768 * 2, 768 * 2)\n",
    "        self.l2 = nn.Linear(768 * 2, 2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            self.drop_out,\n",
    "            self.l1,\n",
    "            nn.LeakyReLU(),\n",
    "            self.l2 \n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        model_output = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, output_hidden_states=True)\n",
    "        hidden_states = model_output[2]\n",
    "        out = torch.cat((hidden_states[-1], hidden_states[-3]), dim=-1)\n",
    "        logits = self.linear_relu_stack(out)\n",
    "        \n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        \n",
    "        start_logits = start_logits.squeeze(-1)\n",
    "        end_logits = end_logits.squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits\n",
    "\n",
    "model = QAModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03820465-6b0b-4e81-8a2c-d29f130c4a03",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def focal_loss_fn(start_logits, end_logits, start_positions, end_positions, gamma):\n",
    "    smax = nn.Softmax(dim=1)\n",
    "    probs_start = smax(start_logits)\n",
    "    inv_probs_start = 1 - probs_start\n",
    "    probs_end = smax(end_logits)\n",
    "    inv_probs_end = 1 - probs_end\n",
    "    \n",
    "    lsmax = nn.LogSoftmax(dim=1)\n",
    "    log_probs_start = lsmax(start_logits)\n",
    "    log_probs_end = lsmax(end_logits)\n",
    "    \n",
    "    nll = nn.NLLLoss()\n",
    "    \n",
    "    fl_start = nll(torch.pow(inv_probs_start, gamma)* log_probs_start, start_positions)\n",
    "    fl_end = nll(torch.pow(inv_probs_end, gamma)*log_probs_end, end_positions)\n",
    "    \n",
    "    return ((fl_start + fl_end)/2)\n",
    "\n",
    "optim = AdamW(model.parameters(), lr=2e-5, weight_decay=2e-2)\n",
    "total_acc = []\n",
    "total_loss = []\n",
    "\n",
    "def train_epoch(model, dataloader, epoch):\n",
    "    ctr = 0\n",
    "    batch_tracker = 0\n",
    "    model = model.train()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    for batch in tqdm(dataloader, desc = 'Running Epoch '):\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        token_type_ids = batch['token_type_ids'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "        out_start, out_end = model(input_ids=input_ids, \n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids)\n",
    "\n",
    "        loss = focal_loss_fn(out_start, out_end, start_positions, end_positions,1)\n",
    "        losses.append(loss.item())\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        \n",
    "        start_pred = torch.argmax(out_start, dim=1)\n",
    "        end_pred = torch.argmax(out_end, dim=1)\n",
    "            \n",
    "        acc.append(((start_pred == start_positions).sum()/len(start_pred)).item())\n",
    "        acc.append(((end_pred == end_positions).sum()/len(end_pred)).item())\n",
    "\n",
    "        batch_tracker = batch_tracker + 1\n",
    "        if batch_tracker==250 and epoch==1:\n",
    "            total_acc.append(sum(acc)/len(acc))\n",
    "            loss_avg = sum(losses)/len(losses)\n",
    "            total_loss.append(loss_avg)\n",
    "            batch_tracker = 0\n",
    "    ret_acc = sum(acc)/len(acc)\n",
    "    ret_loss = sum(losses)/len(losses)\n",
    "    \n",
    "    number_same = (start_pred == start_positions).sum() + (end_pred == end_positions).sum()\n",
    "    precision = 1.0 * number_same / (len(start_pred) + len(end_pred))\n",
    "    recall = 1.0 * number_same / (len(start_positions) + len(end_positions))\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    \n",
    "    return (ret_acc, ret_loss, f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7323e233-38e0-44c7-aede-e7136c9c8745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_model(model, dataloader):\n",
    "    model = model.eval()\n",
    "    losses = []\n",
    "    acc = []\n",
    "    ctr = 0\n",
    "    answer_list=[]\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc = 'Running Evaluation'):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            token_type_ids = batch['token_type_ids'].to(device)\n",
    "            start_true = batch['start_positions'].to(device)\n",
    "            end_true = batch['end_positions'].to(device)\n",
    "            \n",
    "            out_start, out_end = model(input_ids=input_ids, attention_mask=attention_mask,token_type_ids=token_type_ids)\n",
    "\n",
    "            start_pred = torch.argmax(out_start)\n",
    "            end_pred = torch.argmax(out_end)\n",
    "            answer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_pred:end_pred]))\n",
    "            tanswer = tokenizerFast.convert_tokens_to_string(tokenizerFast.convert_ids_to_tokens(input_ids[0][start_true[0]:end_true[0]]))\n",
    "            answer_list.append([answer,tanswer])\n",
    "\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6ac4c4f6-bdaa-4d8f-b8f9-554c302f94a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting taining the model:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Epoch : 100%|██████████| 2320/2320 [05:31<00:00,  7.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.7649098099305711\n",
      "Accuracy: 0.4696197660126049\n",
      "F1: 0.1428571492433548\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [03:21<00:00, 78.80it/s]\n",
      "Running Epoch : 100%|██████████| 2320/2320 [05:30<00:00,  7.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.8806773820763518\n",
      "Accuracy: 0.6680245535897797\n",
      "F1: 0.785714328289032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [03:16<00:00, 80.64it/s]\n",
      "Running Epoch : 100%|██████████| 2320/2320 [05:30<00:00,  7.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.5227112372887546\n",
      "Accuracy: 0.773183497555297\n",
      "F1: 0.785714328289032\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Evaluation: 100%|██████████| 15875/15875 [03:16<00:00, 80.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WER (base model) -  [5.102992125984252, 5.370141732283464, 5.354456692913386]\n"
     ]
    }
   ],
   "source": [
    "from evaluate import load\n",
    "wer = load(\"wer\")\n",
    "EPOCHS = 3\n",
    "model.to(device)\n",
    "wer_list=[]\n",
    "print('Starting taining the model:')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_acc, train_loss, train_f1 = train_epoch(model, train_data_loader, epoch+1)\n",
    "    print(f\"Loss: {train_loss}\")\n",
    "    print(f\"Accuracy: {train_acc}\")\n",
    "    print(f\"F1: {train_f1}\")\n",
    "    \n",
    "    answer_list = eval_model(model, valid_data_loader)\n",
    "    pred_answers=[]\n",
    "    true_answers=[]\n",
    "    for i in range(len(answer_list)):\n",
    "        if(len(answer_list[i][0])==0):\n",
    "            answer_list[i][0]=\"$\"\n",
    "        if(len(answer_list[i][1])==0):\n",
    "            answer_list[i][1]=\"$\"\n",
    "        pred_answers.append(answer_list[i][0])\n",
    "        true_answers.append(answer_list[i][1])\n",
    "    wer_score = wer.compute(predictions=pred_answers, references=true_answers)\n",
    "    wer_list.append(wer_score)\n",
    "\n",
    "print('WER (base model) - ',wer_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82228226-c42f-460a-a3a2-5c5167446554",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
